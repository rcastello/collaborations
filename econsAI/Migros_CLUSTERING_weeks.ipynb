{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "from datetime import timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from helpers_preprocessing import *\n",
    "from helpers_postprocessing import *\n",
    "from helpers_clustering import *\n",
    "from helpers_plot import *\n",
    "from helpers_modelweek import *\n",
    "from helpers_linear_regression import *\n",
    "from building_routine import analyze_building\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import + process/clean building data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small preprocess: before going down the code of the clustering, I re-download the data to have the names of the stations...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(\"data/191126_E3M_LESO_GMOSRawValuesWithRanking.csv\", header=0,encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question to Dan: what is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw dataset\n",
    "data_path = \"data/data.csv\"\n",
    "data_raw = load_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the names of the buildings (change \"Locations\" to 0 to make the rest of the code work...)\n",
    "locations = [0] + list(dat.iloc[2])[1:]\n",
    "\n",
    "# Add at the end of teh rows the location names to keep track of them...\n",
    "data_raw.columns = locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the columns indices that we need to use from external file...\n",
    "data_col = pd.read_csv('data/columns.csv', header=None)\n",
    "# ...and select only columns related to energy consumption\n",
    "# column 0 correspond to measurement time, all the others are single building measurenets\n",
    "energy_consumption_columns = np.concatenate(([0], np.hstack(data_col.values)))\n",
    "\n",
    "# Format raw dataset by:\n",
    "# 1) selecting only columns related to energy consumption \n",
    "# 2) casting index to datetime\n",
    "# 3) removing wrong measurements (duplicated rows, missing time records)\n",
    "# 4) dropping last 6 days in order to have complete weeks\n",
    "data_with_NaNs = format_data(data_raw, energy_consumption_columns)\n",
    "\n",
    "# Discard columns which contains more than max_NaNs, otherwise fill empty values with pd.fillna()\n",
    "# selected_columns is used to link results to the corresponding building in the raw dataset\n",
    "max_NaNs = 100\n",
    "data_no_NaNs, selected_columns = NaN_handling(data_with_NaNs, max_NaNs, energy_consumption_columns)\n",
    "\n",
    "# Discard columns representing small buildings (setting the threshold for the mean energy consupmtion)\n",
    "minimum_mean_energy_consumption = 0\n",
    "data_no_small, selected_columns = remove_small_buildings(data_no_NaNs, minimum_mean_energy_consumption, selected_columns)\n",
    "\n",
    "\n",
    "# ------- KEEP TRACK OF THE LOCATIONS OF EACH BUILDING, in the same order\n",
    "selected_buildings = list(data_no_small.columns)\n",
    "\n",
    "\n",
    "# Rename columns as \"building_\" + N, where N in [0, #columns-1]\n",
    "data_renumbered = renumber_columns(data_no_small)\n",
    "\n",
    "# Rename the processed dataset\n",
    "data = data_renumbered\n",
    "\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selected_buildings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building weekly anomaly detection: Fourier + PCA + DBSCAN clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look at multiple buildings (there are 23x7 weeks data - 23 months from january 2018 until mid november 2019) and:\n",
    "\n",
    "- Apply Fourier (features) + PCA + DBSCAN\n",
    "- Detect : Normal (1st cluster) // Anormal (2nd cluster) // big outliers (outliers)\n",
    "- Check the time series to see if the clusters actually make sense! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"ticks\", font_scale=1, context=\"talk\")\n",
    "sns.set_style(\"white\", {'axes.grid' : False})\n",
    "plt.rcParams['figure.figsize'] = (22, 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_energy_plot(y_min, y_max):\n",
    "    \"\"\"\n",
    "    Utility to format all time series plots\n",
    "    :param double y_min: minimum y values displayed\n",
    "    :param double y_max: maximum y values displayed\n",
    "    \"\"\"\n",
    "    plt.gca().set_ylim([y_min, 1.2*y_max])  # TODO: fix y_max to add also legend\n",
    "    plt.xticks(rotation=20)\n",
    "    plt.gca().set_ylabel('[kWh]')\n",
    "    plt.gca().set_xlabel('')\n",
    "    plt.grid('True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLOT!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make nice plots for each building, while keeping in track the normal and anormal weeks... We have 119 buildings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "building_WeeksTypes_list = []\n",
    "\n",
    "#Â for each building\n",
    "for building_number in range(1,120):\n",
    "    \n",
    "    ##################################################################################################################################\n",
    "    # ----------------------------------------------------- Pre-processing --------------------------------- \n",
    "    ##################################################################################################################################\n",
    "\n",
    "\n",
    "    # ----------- create dataframe for time series ---------------------------\n",
    "    # All the time series in our dataset have 15 minutes resolution, resulting in 96 measurements per day.\n",
    "    data_per_day = 96\n",
    "    data_per_week = data_per_day*7\n",
    "\n",
    "    energy_consumptions, total_weeks = create_single_building_dataframe(data, building_number, data_per_week)\n",
    "    #energy_consumptions.head(10)\n",
    "\n",
    "    ##################################################################################################################################\n",
    "    # ----------------------------------------------------- PCA + DBSCAN --------------------------------- \n",
    "    ##################################################################################################################################\n",
    "\n",
    "    # ----------- Fourier + PCA + DBSCAN ---------------------------\n",
    "    # Value used to define the epsilon parameter of DBSCAN, obtained\n",
    "    # through grid search (see notebook grid_search_DBSCAN)\n",
    "    first_singular_value_multiplier = 0.012\n",
    "    # Analysis performed in 2 dimension\n",
    "    principal_components_number = 2\n",
    "    # FT + PCA on selected building\n",
    "    PCA_weeks, S = compressed_week_representation(energy_consumptions, \n",
    "                                              principal_components_number, \n",
    "                                              data_per_week, \n",
    "                                              total_weeks)\n",
    "    # Run DBSCAN to identify the clusters in the space defined by the Principal Components\n",
    "    epsilon = S[0]*first_singular_value_multiplier\n",
    "    clustering = DBSCAN(eps=epsilon, min_samples=9).fit(PCA_weeks.T)\n",
    "    # Assign each week to a cluster and reorder clusters based on their cardinality\n",
    "    # the bigger one contains normal weeks and is plotted in blue; the remaining clusters and outliers (black)\n",
    "    # are the atypical weeks\n",
    "    ordered_clusters, outliers = extract_clusters(clustering)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##### extract classes of weeks\n",
    "    normal_weeks, atypical_weeks,outliers_weeks = weeks_clustering(ordered_clusters,outliers,total_weeks)\n",
    "    \n",
    "    building_WeeksTypes_list.append((normal_weeks, atypical_weeks,outliers_weeks))\n",
    "    \n",
    "    \n",
    "    ##################################################################################################################################\n",
    "    # ----------------------------------------------------- PLOTS --------------------------------- \n",
    "    ##################################################################################################################################\n",
    "\n",
    "\n",
    "    # actual location\n",
    "    \n",
    "    loc = selected_buildings[building_number-1]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    # time series values to set up the subplots\n",
    "    data_per_group = data_per_week*4\n",
    "    group_range = [0, int(total_weeks/4)]\n",
    "    week_indices = np.arange(0, total_weeks)\n",
    "\n",
    "    # -------------------- CLUSTER points ----------------------------------------\n",
    "    max_cluster_index = np.max(clustering.labels_)\n",
    "\n",
    "    plt.subplot(int(group_range[1]/3) + 1 ,1, 1)\n",
    "\n",
    "    for cluster_index in range(0, max_cluster_index+1):\n",
    "        plt.scatter(PCA_weeks[0, ordered_clusters[cluster_index]], PCA_weeks[1, ordered_clusters[cluster_index]])\n",
    "    plt.scatter(PCA_weeks[0, outliers], PCA_weeks[1, outliers], color='black')\n",
    "    if building_number != None:\n",
    "        plt.title('Building ' + str(building_number) + \" (\" + loc + \") \",fontsize=30)\n",
    "    plt.gca().set_xlabel('PC1')\n",
    "    plt.gca().set_ylabel('PC2')\n",
    "    plt.tick_params(\n",
    "        axis='x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelbottom=False)\n",
    "    #plt.show()\n",
    "\n",
    "    # -------------------- Time series ----------------------------------------\n",
    "\n",
    "    #plt.figure(figsize=(20, 4))\n",
    "    for group_index in range(group_range[0], group_range[1]):\n",
    "\n",
    "        plt.subplot(int(group_range[1]/3)+1,3,group_index + 4)\n",
    "        for local_week_index in range(0, 4):\n",
    "            global_week_index = group_index*4 + local_week_index\n",
    "            # Look for the current week in every cluster (plot with relative color)\n",
    "            found_in_cluster = False\n",
    "            for cluster_index in range(0, len(ordered_clusters)):\n",
    "                if (global_week_index in week_indices[ordered_clusters[cluster_index]]):\n",
    "                    week_color = 'C' + str(cluster_index)\n",
    "                    energy_consumptions.loc[energy_consumptions['week'] == global_week_index, 'consumption']\\\n",
    "                                       .plot(grid='True',rot=45, color=week_color)\n",
    "                    found_in_cluster = True\n",
    "            # If week not found then it's outlier (plot in black)\n",
    "            if not found_in_cluster:\n",
    "                energy_consumptions.loc[energy_consumptions['week'] == global_week_index, 'consumption']\\\n",
    "                                   .plot(grid='True', rot=45, color='black')\n",
    "        plt.title('4Weeks Group ' + str(group_index))\n",
    "    \n",
    "        format_energy_plot(energy_consumptions.consumption.min(),energy_consumptions.consumption.max())\n",
    "        \n",
    "    #plt.show()\n",
    "    plt.subplots_adjust(hspace=0.9)\n",
    "    #plt.tight_layout()\n",
    "\n",
    "    # save\n",
    "    fig.savefig(\"\".join([\"MigrosBuildings_Weeks_DBSCAN_and_TimeSeries/Weeks_Points_and_TimeSeries_building_\",str(building_number),\".pdf\"]), dpi=300, box_inches= \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[el for el in range(119,120)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We extracted the normal, anormal, and outlier weeks for each building, while plotting.  (NOTE that the outliers are also listed in the anormal points)\n",
    "\n",
    "Now we can form it in a nice table, giving all weeks, specifying if each week is normal or not... Normal:1 / Anormal: 0 / Outlier:-1 / "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "week_building_list = []\n",
    "\n",
    "for building_weeks in building_WeeksTypes_list:\n",
    "    #Â types of weeks    \n",
    "    normal, anormal, outliers = set(building_weeks[0]),  set(building_weeks[1]),  set(building_weeks[2])\n",
    "    #print(len(normal)+len(anormal)+len(outliers))\n",
    "    week_list = []\n",
    "    # we say: \n",
    "    for week_i in range(98):\n",
    "        if week_i in normal:\n",
    "            week_list.append(1)\n",
    "        else:\n",
    "            if week_i in outliers:\n",
    "                week_list.append(-1)\n",
    "            else:\n",
    "                week_list.append(0)\n",
    "\n",
    "    week_building_list.append(week_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nice table\n",
    "building_weeks_MIGROS = pd.DataFrame(np.array(week_building_list))\n",
    "# tu put time in index, buildings in columns\n",
    "building_weeks_MIGROS = building_weeks_MIGROS.transpose()\n",
    "\n",
    "#Â columns = locations\n",
    "building_weeks_MIGROS.columns = selected_buildings\n",
    "#Â index = week index\n",
    "\n",
    "# Extract week end data \n",
    "loul = data.resample(\"1W\", how=\"sum\")\n",
    "WEEKS_END = loul.index\n",
    "building_weeks_MIGROS.index = [el + timedelta(days=-6) for el in WEEKS_END] \n",
    "building_weeks_MIGROS.index.name = \"Week_start\"\n",
    "\n",
    "building_weeks_MIGROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the data in Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_weeks_MIGROS.to_excel(\"Anomaly_detection_Weeks_Migros_buildings.xlsx\",\n",
    "             sheet_name='Sheet_name_1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a mesh, just for fun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(building_weeks_MIGROS)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: some buildings have a lot of outliers. \n",
    "Reasons? \n",
    "\n",
    "It's maybe because the anomalies can be like a missing day but also much higher or small values in the demand --> Seasonality? \n",
    "But then why doesnt it create another clusters if there are a lot of similar outliers ???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
